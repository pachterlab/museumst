---
title: "Untitled"
author: "Lambda Moses"
date: "6/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

To do for all notebooks: Explain what the code does, just as I did in the BBB notebook.
```{r}
library(tidyverse)
library(lubridate)
library(preText)
library(quanteda)
library(stm)
library(text2vec)
library(lexvarsdatr)
library(ggrepel)
library(sf)
library(furrr)
library(stminsights)
library(uwot)
library(tidytext)
library(broom)
library(gam)
library(igraph)
library(ggraph)
library(bluster)
library(tidygraph)
devtools::load_all()
plan(multisession)
theme_set(theme_bw())
```

```{r}
data("lcm_abstracts")
data("lcm_city_gc")
```

```{r}
pubs_on_map(lcm_abstracts, lcm_city_gc, plot = "hexbin", label_insts = FALSE, 
            label_cities = TRUE, n_label = 7)
```

Not sure if this is a good idea, since it's plotting number of publications per unit area. I think per capita is confusing, again, because researchers move a lot and I'm not sure who are the "locals". Is per unit area more reasonable? Maybe. It helps when there's overplotting in the scatter plot.
```{r}
pubs_on_map(lcm_abstracts, city_gc = lcm_city_gc, label_cities = TRUE, n_label = 7)
```

```{r}
pubs_per_cat(lcm_abstracts, country, n_top = 20)
```

```{r}
pubs_per_cat(lcm_abstracts, city, n_top = 20)
```

I can try to get city population data, though a complication is that some city names have different spellings and sometimes there are multiple cities with the same name (e.g. trust me, there are Los Angeles's outside California). Well, I can geocode those cities with the Google API and extract Google's standards. But I don't think this is all that important, so I'm not doing it right now.

Unfortunately, I can't check the institutions here since I haven't figured out a way to automatically extract the institution names and standardize them from the address given that the addresses here do not have a standardized format. OK, given more time, I probably can figure that out, but I'd rather not do it for now. But the cities do tell a lot. Los Angeles means either UCLA or USC or some hospitals like Cedars Sinai. Ithaca means Cornell. New York means Columbia, NYU, Cornell medical school, or Rockefeller. Boston means Harvard Medical School or maybe Boston College or Boston University. Bethesda means NIH. Again, all the usual suspects.

```{r}
pubs_per_capita(lcm_abstracts)
```

```{r, fig.width=6, fig.height=10}
pubs_per_capita(df_1st, plot = "bar")
```

That's very different from other areas of spatial transcriptomics.

```{r}
pubs_on_map(lcm_abstracts, lcm_city_gc, "europe", label_cities = TRUE)
```

```{r}
pubs_per_capita(lcm_abstracts, "europe")
```

```{r}
pubs_per_capita(lcm_abstracts, "europe", plot = "bar")
```

Denmark never showed up for the other areas in spatial transcriptomics. 

```{r}
pubs_on_map(lcm_abstracts, lcm_city_gc, "usa", label_cities = TRUE)
```

Orangeburg. Never heard about it. It's the Nathan Kline Institute, which again, I've never heard of. I think this shows how LCM is different from other fields of spatial transcriptomics; the other fields are more confined to the very elite institutions.
```{r}
pubs_per_capita(lcm_abstracts, "usa")
```

```{r}
pubs_per_capita(lcm_abstracts, "usa", plot = "bar")
```

I think that's because of NIH's labs in Bestheda and Baltimore. Apparently NIH has a lot interest in LCM, though not as much in other fields of spatial transcriptomics compared to some other most elite institutions. Anyway, I don't think this says too much about those states, since we know that academics are kind of like modern day foragers not anchored in a city early in their careers. Since the same people move around, I don't think this data says too much about the states themselves per se since the people doing the research are quite likely to be from somewhere else. I'm not convinced that Kansas and Virginia are really better than California in this field. But I still don't think I should plot the raw numbers on the choropleth, since it creates an illusion that physically larger states have more publications than they actually do just because they're physically larger.

How about over time?
```{r}
pubs_per_year(lcm_abstracts, binwidth = 150)
```

This is not manually curated; it's just whatever PubMed gave me from a search. Curiously, there's a first peak, and a rising and taller second peak. I wonder why. When text mining, I can try to compare the two peaks.

I'll text mine the titles, keywords, or maybe abstracts as well. I can also try to extract country, state, city, institution, and department from the address field.

# Text mining abstracts

I think the city (a convenient proxy of institution) and journal have a lot to do with the topic of an abstract. But meanwhile, if a lot of cities and journal only have 1 or 2 abstracts, then it makes the model unnecessarily complex given the relatively small corpus and won't make it more predictive. I know, I fit this model for explanation of importance of the covariates, but at least the model should capture the important features and not too much noise, in other words, not overfit, for me to trust the explanation. 
```{r}
length(unique(lcm_abstracts$city2))
```

```{r}
length(unique(lcm_abstracts$journal))
```

```{r}
lcm_abstracts %>% 
  count(city2) %>% 
  ggplot(aes(n)) +
  stat_ecdf() +
  labs(x = "Number of publications per city", y = "F") +
  scale_x_continuous(breaks = scales::breaks_width(2)) +
  scale_y_continuous(breaks = scales::breaks_width(0.2))
```

By lumping everything below 4 as "Other", there're only about 30% of cities left.

Another question: What percent of cities have what percent of publications? Rather than an ECDF of the number of publications per city, I can plot the cumulative number/proportion of all publications vs. rank of city. 
```{r}
df <- lcm_abstracts %>% 
  count(city2) %>% 
  arrange(desc(n)) %>% 
  mutate(rank = row_number(desc(n)),
         rel_rank = rank/max(rank),
         cum_n = cumsum(n),
         cum_prop = cum_n/sum(n))
ggplot(df, aes(rel_rank, cum_prop)) +
  geom_line() +
  scale_x_continuous(breaks = scales::breaks_width(0.2), labels = scales::percent) +
  scale_y_continuous(breaks = scales::breaks_width(0.2), labels = scales::percent) +
  labs(x = "City percentile", y = "Percent of publications")
```

When I do lump cities with fewer than 3 publications, the remaining 30% of cities have 70% of publications. But is it helpful at all that a bit over 30% of publications are in the "Other" category? Well, maybe better than not using cities at all, since given that in practice, topics have a lot to do with the interests of individual labs, it makes sense to include city as a covariate. 
```{r}
lcm_abstracts %>% 
  filter(journal != "bioRxiv") %>% 
  count(journal) %>% 
  ggplot(aes(n)) +
  stat_ecdf() +
  labs(x = "Number of publications per journal", y = "F") +
  scale_x_continuous(breaks = scales::breaks_width(2)) +
  scale_y_continuous(breaks = scales::breaks_width(0.2))
```

By lumping journals with fewer than 3 papers into "Other", there're about 25% of journals left.
```{r}
df <- lcm_abstracts %>% 
  filter(journal != "bioRxiv") %>% 
  count(journal) %>% 
  arrange(desc(n)) %>% 
  mutate(rank = row_number(desc(n)),
         rel_rank = rank/max(rank),
         cum_n = cumsum(n),
         cum_prop = cum_n/sum(n))
ggplot(df, aes(rel_rank, cum_prop)) +
  geom_line() +
  scale_y_continuous(breaks = scales::breaks_width(0.2), labels = scales::percent) +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "Journal percentile", y = "Percent of publications")
```

The remaining top 25% of journals have 65% of papers. 

This curve rises more sharply than the city curve. 
```{r}
abstracts <- lcm_abstracts %>% 
  mutate(city_eff = fct_lump_min(city2, 5),
         journal_eff = fct_lump_min(journal, 5))
```

```{r}
ac <- corpus(abstracts, docid_field = "title", text_field = "abstract")
```

```{r}
n_words <- summary(ac, n = Inf)
```

```{r}
hist(n_words$Tokens)
```

```{r}
hist(n_words$Types)
```

```{r}
hist(n_words$Sentences)
```

Did the number of words in abstracts change over time?
```{r}
ggplot(n_words, aes(date_published, Tokens)) +
  geom_point() +
  geom_smooth(method = "lm")
```

Nope.

# Tuning stm model
I'm debating whether I should stem the words. Apparently here this doesn't really matter. I don't know any keyword that has multiple meanings in the context of LCM. Anyway, there're more burning questions like whether I should remove punctuation. I don't think it matters in this context, but just to check. I'll randomly pick 200 abstracts and use the `preText` package to get a sense of the effects of those preprocessing choices.

```{r}
ac_sub <- corpus_sample(ac, size = 200)
```

```{r}
ac_fp <- factorial_preprocessing(ac_sub, parallel = FALSE, 
                                 intermediate_directory = ".", 
                                 return_results = FALSE,
                                 infrequent_term_threshold = 0.005)
```

```{r}
head(ac_fp$choices)
```

```{r}
preText_results <- preText(
  ac_fp,
  dataset_name = "LCM abstracts",
  distance_method = "cosine",
  num_comparisons = 50)
```

```{r, fig.width=6, fig.height=8}
preText_score_plot(preText_results)
```

The options with lower scores have less unusual results.
```{r}
regression_coefficient_plot(preText_results,
                            remove_intercept = TRUE)
```

Here this means removing stopwords and punctuation makes the results less likely to be unusual, while removing infrequent terms may lead to more unusual results, while the other options might not significantly affect the results. Here preText only looked at how these preprocessing choices influence pairwise distances between abstracts. I see how this is relevant to political science, for which that package was written. How relevant is pairwise distance to LCM abstracts? 

I think in the first trial, I'll only use unigrams, do stem the words, do remove stopwords and punctuation, do remove numbers, and do turn everything into lower case, for the simplest model. Then I can try what if I don't remove stopwords, what if I don't remove punctuation, and what if I do remove infrequent words and see how that would affect the topic modeling results.

What are the collocations, i.e. common phrases? 
```{r}
ac_col <- tokens(ac, split_hyphens = TRUE, remove_symbols = TRUE) %>% 
  tokens_remove(c(stopwords(), "sup", "alt")) %>% 
  tokens_remove(c("^c_", "^m_", "^o_"), valuetype = "regex") %>% 
  tokens_wordstem() %>% 
  textstat_collocations(min_count = 20, size = 2:4) %>% 
  filter(count_nested < count) %>% 
  arrange(desc(count)) 
```

```{r}
ac_col
```

OK, now I know which phrases are common. While many of the phrases here are relevant and are usually used as if they were words, some are not, so I'll use a list of phrases based on my background.
```{r}
phrases <- str_split(ac_col$collocation, pattern = " ", simplify = FALSE)
```

```{r}
ac_tokens <- tokens(ac, remove_punct = TRUE, 
                    remove_symbols = TRUE, remove_url = TRUE,
                    split_hyphens = TRUE) %>% 
  tokens_tolower(keep_acronyms = TRUE) %>% 
  tokens_remove(c(stopwords(), "laser", "capture", "microdissection", "use", 
                  "using", "used", "uses", "LCM", "sup", "LMD", "gene", "genes",
                  "width", "height", "src", "figdir", "alt")) %>% 
  tokens_remove(c("^c_", "^m_", "^o_"), valuetype = "regex") %>% 
  tokens_wordstem() %>% 
  tokens_compound(phrases)
```

```{r}
lcm_tokens <- ac_tokens
usethis::use_data(lcm_tokens, overwrite = TRUE)
```

```{r}
ac_dfm <- dfm(ac_tokens)
```

```{r}
ac_dfm2 <- dfm_trim(ac_dfm, min_docfreq = 0.001, docfreq_type = "prop")
```

Here spectral means using NMF for initialization. I debated for a while whether I should use the covariates. I eventually decided that I should. Otherwise it's just an intercept. At worst the effects of the covariates are not significant. But I do think some of the covariates are relevant to topic prevalence, such as journal -- the journal Plant Cell obviously is about plants and not about human cancer. OK, it seems that adding the journal covariate only hurts the held out likelihood whether I lump low frequency journals into "Other" or not, so I'll not use it. I lumped cities with fewer than 5 publications into "Other", since using smaller numbers hurts the held-out likelihood, but I still do want to see the effects of city so I don't want to lump too much. Over 80% of cities and about half of publications are now in "Other", so this only shows the effects of the top 15% or so cities.

```{r}
k_result2 <- my_searchK(ac_dfm2, K = seq(5, 60, by = 5), 
                       prevalence = ~ date_num + city_eff,
                       gamma.prior = "L1",
                       verbose = FALSE, seed = 1891)
```

```{r}
plot_k_result(k_result2)
```

Looks like the number of topics should be 25 to 40 according to the residuals, but held-out likelihood and semantic coherence. suggest a smaller number. The "residuals" here means dispersion of the residuals, and ideally it should be 1. 

```{r}
plot_ec(k_result2, seq(30, 60, by = 10))
```

As expected, fewer topics results into higher semantic coherence and lower exclusivity. This is really a trade off. Sort of a compromise, here I choose the model with K = 50. Since I have held out part of the data when choosing K, I'll use this K to fit the model with the full data.

```{r}
stm_res <- stm(ac_dfm2, K = 50, prevalence = ~ date_num + city_eff,
               gamma.prior = "L1", seed = 1919)
```

```{r, fig.width=6, fig.height=5}
plot(stm_res)
```

Upon manual inspection, 50 topics, with the spline for date, seem to give more reasonable results.
```{r}
saveRDS(k_result2, "k_result_lcm.rds")
saveRDS(stm_res, "stm_res_lcm.rds")
```

Tidyverse version: Again, I'm copying and pasting code from Julia Silge's blog. I learnt a lot about text mining from her. However, I don't like her colorful barcharts, so I changed her code to make the lollipop plot here.

```{r}
findThoughts(stm_res, lcm_abstracts$abstract, 50, n = 10, thresh = 0.9)
```

```{r, fig.width=6, fig.height=9}
td_beta <- tidy(stm_res)
td_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  mutate(topic = paste0("Topic ", topic),
         topic = fct_relevel(topic, paste0("Topic ", 1:50)),
         term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta)) +
  geom_segment(aes(xend = term), yend = 0, show.legend = FALSE) +
  geom_point(color = "blue") +
  facet_wrap(~ topic, scales = "free_y", ncol = 5) +
  coord_flip() +
  scale_x_reordered() +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  labs(x = NULL, y = expression(beta),
       title = "Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics")
```

1. Stem cell and fetal development
2. GWAS, genetic screens, and genetics of complex phenotypes
3. Biomechanics, ECM, eye lens, muscles, and morphogenesis
4. Data analysis, especially of RNA-seq, but also of 3D genome structure and microarray
5. miRNAs in cancer
6. Quantitative analyses of cancer, clinical and bioinformatic
7. Hippocampus and Alzheimer's disease, sometimes related to Down syndrome
8. Prostate cancer and other stuff in molecular biology and biochemistry, probably because some prostate cancer papers have an emphasis on molecular biology
9. Plant embryos, plant development, and some stuff about evolution and ecology related to plants
10. Proteomics, especially in cancer
11. Cancer progression and diagnostics, especially lung cancer
12. Inflammation and immunology, especially in skin diseases
13. Breast cancer and liver cancer, with an emphasis in data analysis
14. Neural circuitry, neural plasticity, brain injury, and behavior
15. Plant gamitogenesis and reproduction
16. Spasmolytic polypeptide-expressing metaplasia (SPEM), oncogenes, KRAS
17. Endometrium and implantation. Somehow the top 2 entries are about hearing loss. Why? Epithelium?
18. Cell cycle, also hepatic zonation and circadian rhythm (the latter is also a cycle, so...)
19. Neurons, especially dopaminergic
20. Tumor stroma and microenvironment
21. Plant roots
22. Intestine, especially microbiome and immune response
23. Hypothalamus, obesity, and appetite
24. PDAC, and some stuff about glioma and prostate cancer
25. ALS, and other neurodegenerative diseases affecting motor neurons
26. Epigenetics
27. Tumor single cell profiling and cellular heterogeneity
28. Tissue isolation and preparation
29. Bone growth plate, especially recovery after radiotherapy, and some other stuff like oocytes, glaucoma, and epithelial injury
30. Pancreas and diabetes, especially T2D
31. Lymphocytes, lymphatic and blood vessels
32. Prefrontal cortex and schizophrenia
33. Synapses, dendritic spines, neuron potentiation, sometimes related to memory
34. Cancer genomics, mutations, and phylogeny
35. Bone formation, but also some other stuff about cancer and kidneys
36. Neurodegenerative diseases, Alzheimer's, Parkinson's, and multiple system atrophy
37. Spatial single cell techniques and imaging
38. Connective tissues and ECM, and some other stuff about circadian rhythms
39. Stem cells and development
40. Plant seed development and reproduction
41. RNA extraction and amplification, especially in microarray, but also in RNA-seq
42. Lots of different stuff about epithelium
43. Plant leaves, but also other stuff about gamitogenesis
44. Cancer pathway analyses and molecular and cellular mechanisms
45. Plant nitrogen fixation and soil microbiome
46. Lots of different stuff related to fibrosis and fibroblasts, such as in lung diseases and graft rejection
47. Neuron morphogenesis, axon guidance, somehow also angiogenesis, protein signaling
48. Inflammation, immune response, especially in atherosclerosis, though there's some other stuff about blood vessels
49. Model organisms and in vitro model systems
50. Intrahepatic cholangiocarcinoma (ICC)

```{r}
topic_types <- tribble(~ category, ~ topic,
                       "Cancer", c(5,6,8,10,11,13,16,20,24,27,34,44,50),
                       "Botany", c(9,15,21,40,43,45),
                       "Neuroscience", c(7,14,19,23,25,32,33,36,47),
                       "Development", c(1,3,17,18,29,35,39),
                       "Immunology", c(12,22,48),
                       "Technical", c(4,28,37,41))
```

```{r}
topic_types <- topic_types %>% 
  unnest(cols = "topic")
```

```{r}
topics_other <- tibble(category = "Other",
                       topic = setdiff(seq_len(50), topic_types$topic))
topic_types <- bind_rows(topic_types, topics_other)
topic_types <- topic_types %>% 
  mutate(topic = factor(topic, levels = 1:50))
```

```{r}
lcm_dfm2 <- ac_dfm2
usethis::use_data(lcm_dfm2, overwrite = TRUE)
usethis::use_data(stm_res, overwrite = TRUE)
```


# Topics over time
```{r}
effs <- estimateEffect( ~ date_num, stm_res, 
                       metadata = docvars(ac_dfm2))
```

```{r}
effs_df <- get_effects(effs, "date_num", type = "continuous")
effs_df <- effs_df %>% 
  mutate(topic = fct_relevel(topic, as.character(1:50)),
         date_published = as_date(value))
effs_df <- effs_df %>% 
  left_join(topic_types, by = "topic")
```

```{r, fig.width=6, fig.height=7}
ggplot(effs_df, aes(date_published, proportion)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.5, fill = "gray") +
  geom_line(aes(color = category)) +
  facet_wrap(~ topic, ncol = 5) +
  coord_cartesian(ylim = c(0, 0.4)) +
  theme(legend.position = "top")
```

Again, as expected, many cancer related topics declined in proportion.

Just to inspect to verify the above results
```{r}
abstracts <- cbind(abstracts, as.data.frame(stm_res$theta))
```

```{r}
year_theta_long <- abstracts %>% 
  mutate(date_bin = floor_date(date_published, unit = "year")) %>% 
  select(date_bin, V1:V50) %>% 
  pivot_longer(cols = V1:V50, names_to = "topic", values_to = "theta") %>% 
  mutate(topic = str_remove(topic, "^V"),
         topic = fct_relevel(topic, as.character(1:50)))
year_means <- year_theta_long %>% 
  group_by(date_bin, topic) %>% 
  summarize(means = mean(theta)) 
```

```{r}
topics_insp <- as.character(c(11,14,15,29,41,45))
```

```{r}
year_means %>% 
  filter(topic %in% topics_insp) %>% 
  ggplot(aes(date_bin, means)) +
  geom_line() +
  facet_wrap(~ topic) +
  theme(legend.position = "none") +
  geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs", k = 3))
```

OK, so the stuff I got from `estimateEffects` looks real, rather than artefacts from smoothing. But this still doesn't answer the question why these topics have lower prevalence. Here I plotted the mean theta per year for those topics. But the mean can be dragged down if there are more papers over all in that year not because these topics declined, but because other topics grew. So I'll make another plot for how the number of papers with theta over 0.5 changed over time.
```{r}
year_counts <- year_theta_long %>% 
  filter(theta > 0.5) %>% 
  count(date_bin, topic)
```

```{r}
year_counts %>% 
  filter(topic %in% topics_insp) %>% 
  ggplot(aes(date_bin, n)) +
  geom_line() +
  facet_wrap(~ topic) +
  theme(legend.position = "none") +
  geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs", k = 3))
```

When the count is used, the increases became more prominent while the decreases and bulges became less prominent. Probably that means the drastic increase in the number of papers of some topics led to decline in proportion in other topics. I should show this plot in the book.

```{r}
effs_summ <- tidy(effs)
effs_summ <- effs_summ %>% 
  filter(!str_detect(term, "Intercept")) %>% 
  mutate(p_val_adj = p.adjust(p.value, method = "BH"))
```

```{r}
effs_summ %>% arrange(p.value)
```

These are the significant ones:

2. GWAS, genetic screens, and genetics of complex phenotypes
4. Data analysis, especially of RNA-seq, but also of 3D genome structure and microarray
6. Quantitative analyses of cancer, clinical and bioinformatic
8. Prostate cancer and other stuff in molecular biology and biochemistry, probably because some prostate cancer papers have an emphasis on molecular biology
9. Plant embryos, plant development, and some stuff about evolution and ecology related to plants
10. Proteomics, especially in cancer
11. Cancer progression and diagnostics, especially lung cancer
13. Breast cancer and liver cancer, with an emphasis in data analysis
14. Neural circuitry, neural plasticity, brain injury, and behavior
18. Cell cycle, also hepatic zonation and circadian rhythm (the latter is also a cycle, so...)
19. Neurons, especially dopaminergic
27. Tumor single cell profiling and cellular heterogeneity
28. Tissue isolation and preparation
29. Bone growth plate, especially recovery after radiotherapy, and some other stuff like oocytes, glaucoma, and epithelial injury
33. Synapses, dendritic spines, neuron potentiation, sometimes related to memory
34. Cancer genomics, mutations, and phylogeny
36. Neurodegenerative diseases, Alzheimer's, Parkinson's, and multiple system atrophy
37. Spatial single cell techniques and imaging
38. Connective tissues and ECM, and some other stuff about circadian rhythms
39. Stem cells and development
40. Plant seed development and reproduction
41. RNA extraction and amplification, especially in microarray, but also in RNA-seq
43. Plant leaves, but also other stuff about gamitogenesis
45. Plant nitrogen fixation and soil microbiome
47. Neuron morphogenesis, axon guidance, somehow also angiogenesis, protein signaling
49. Model organisms and in vitro model systems

# Topics per city

It seems that this drops faster than exponentially. 

Unfortunately, there're too many cities and if I include all of them, the covariance matrix will be singular. So I'll begin with the effects of countries, not considering date published. I know, this is problematic, since there are international collaborations and I don't think topic of interest is spatially autocorrelated. I have lumped cities with too few publications into "other.

```{r}
effs_loc <- estimateEffect( ~ city_eff, stm_res, 
                            metadata = abstracts)
```

```{r}
effs_loc_summ <- tidy(effs_loc)
effs_loc_summ <- effs_loc_summ %>% 
  mutate(p_val_adj = p.adjust(p.value))
effs_loc_summ %>% 
  arrange(p.value)
```

I think in some cases this actually helps identifying institutions (actually I would say labs) that have a special interest in a topic, though often this gives results that are probably spurious, like when a city appears twice for a topic, it gets significant. This also misses some cities that are lumped into "Other". 

```{r}
effs_df_loc <- get_effects(effs_loc, "city_eff", type = "pointestimate")
effs_df_loc <- effs_df_loc %>% 
  mutate(topic = fct_relevel(topic, as.character(1:50)),
         mid = (upper + lower)/2)
```

```{r}
effs_df_loc %>% 
  filter(topic == 45) %>% 
  mutate(value = fct_reorder(value, mid, .desc = FALSE)) %>% 
  top_n(10, mid) %>% 
  ggplot(aes(y = value)) +
  geom_errorbar(aes(xmin = lower, xmax = upper))
```

```{r, fig.width=6, fig.height=6}
heatmap(stm_res$theta)
```

```{r}
topic_cor <- topicCorr(stm_res, method = "huge")
plot(topic_cor)
```

This doesn't look very good. I'll use ggigraph to color the nodes with category.
```{r}
topic_cor_graph <- graph_from_adjacency_matrix(topic_cor$posadj, mode = "undirected")
topic_cor_graph <- as_tbl_graph(topic_cor_graph)
```

```{r}
topic_cor_graph <- topic_cor_graph %>% 
  activate(nodes) %>% 
  mutate(topic = factor(1:50, 1:50)) %>% 
  left_join(topic_types, by = "topic")
```

```{r}
ggraph(topic_cor_graph, layout = "fr") +
  geom_edge_fan(color = "gray50") +
  geom_node_point(aes(color = category), size = 6) +
  scale_color_discrete(name = "Category") +
  geom_node_text(aes(label = topic), color = "white")
```

Here, as expected, the cancer topics and some of the bad topics that have something to do with cancer are clustered together, and the botany ones and the neuroscience ones form their own cluster. 

# Term co-occurance
```{r}
ac_fcm <- fcm(ac_tokens, context = "window", count = "weighted", weights = 1/(1:5))
```

```{r}
glove <- GlobalVectors$new(rank = 125, x_max = 10)
```

```{r}
ac_gv <- glove$fit_transform(ac_fcm, n_iter = 50)
```

```{r}
ac_context <- glove$components
ac_vectors <- ac_gv + t(ac_context)
```

```{r}
dim(ac_vectors)
```

Just to inspect the nearest neighbors of some keywords to see if this embedding makes sense
```{r}
kws <- c("microarray", "plant", "cancer", "neuron")
nns <- map_dfr(kws, lvdr_quick_cosine, tfm = ac_vectors, include_targ = TRUE)
```

```{r}
nns
```

Yes, it does make sense.

There're many words here, so I'll only plot the most frequent ones.
```{r}
wf <- textstat_frequency(ac_dfm)
words_plot <- wf$feature[wf$frequency > 30]
```

```{r}
ac_vectors_select <- ac_vectors[rownames(ac_vectors) %in% words_plot,]
```

```{r}
sim_mat <- sim2(ac_vectors_select)
```

```{r}
mds_res <- cmdscale(1-sim_mat, k = 4, eig = TRUE) 
mds_mat <- mds_res$points %>% 
  data.frame() %>% 
  mutate(label = rownames(sim_mat))
```

```{r}
mds_mat <- mds_mat %>% 
  left_join(wf, by = c("label" = "feature"))
```

```{r}
mds_plt <- mds_mat %>% 
  filter(rank <= 150)
```

```{r}
ggplot(mds_mat, aes(X1, X2)) +
  geom_point(aes(color = docfreq)) +
  scale_color_distiller(palette = "Blues", direction = 1)
```

I'll also cluster the words in glove space.
```{r}
knn_graph <- makeKNNGraph(ac_vectors_select, k = 15)
```

```{r}
set.seed(2021)
clusts <- cluster_louvain(knn_graph)
```

```{r}
clusts_df <- tibble(label = rownames(ac_vectors_select),
                    cluster = clusts$membership)
mds_plt2 <- mds_plt %>% 
  left_join(clusts_df, by = "label")
mds_plt2 <- mds_plt2 %>% 
  mutate(cluster = factor(cluster, 1:9))
```

```{r}
mds_plt2 %>% 
  ggplot(aes(X1, X2, color = cluster)) +
  geom_point() +
  geom_text_repel(aes(label = label), segment.alpha = 0.3, max.overlaps = Inf)
```

Here both the clustering and the MDS separate biological terms from technical terms. The magenta cluster is for technical words, while the other clusters are more biological.

```{r}
mds_plt2 %>% 
  ggplot(aes(X3, X4, color = cluster)) +
  geom_point() +
  geom_text_repel(aes(label = label), segment.alpha = 0.3, max.overlaps = Inf)
```

Here the blue, green, and turquoise clusters are better separated. The blue cluster is about biological functions. Green cluster (3) is clinical. Turquoise (5) seems to be words discussing results. We don't see many words from some clusters because not all words are plotted.

```{r}
feat <- names(topfeatures(ac_fcm, 80))
fcm2 <- fcm_select(ac_fcm, pattern = feat)
set.seed(19)
textplot_network(fcm2, min_freq = 0.9, vertex_labelsize = log((rowSums(fcm2) + 1)/(min(rowSums(fcm2)) + 1)))
```

```{r}
set.seed(2021)
ac_umap <- as.data.frame(umap(ac_vectors[rownames(ac_vectors) %in% mds_plt2$label,]))
ac_umap$label <- rownames(ac_vectors[rownames(ac_vectors) %in% mds_plt2$label,])
ac_umap$cluster <- mds_plt2$cluster
```

```{r}
ggplot(ac_umap, aes(V1, V2, color = cluster)) +
  geom_point() +
  geom_text_repel(aes(label = label), segment.alpha = 0.3, max.overlaps = Inf)
```

Well, umap did a better job than MDS. Well, different perspectives, I suppose.

# Word prevalence

Then see how word/phrase usage changed over time. How to do that? I'm using the method in the slingshot vignette to find genes whose expression changed through time. Problem: documents are not like cells. Because of different topics, some terms may not be mentioned at all in some documents. Well then why not split the documents by topics first and then model the number of times words are used within each document? However, what I'm interested in is not how many times words are used within each document, but how, consider this entire LCM corpus, word use changes over time, as a measure of changes in diction and interests. For instance, say there can be a lot of interest in cancer since a large proportion of the corpus is about cancer, though each individual document doesn't necessarily use the word "cancer" a lot. So I'll bin by year (which seems to be a sensible time scale here) and use the proportion of words in the year. Again, I'm copying some of Julia's code. Hmm, they joke a lot about copying and pasting code from Stack Overflow, but actually I don't use Stack Overflow that much.
```{r}
ac_dfm2_tidy <- tidy(ac_dfm2)
# Add the metadata
ac_dfm2_tidy <- ac_dfm2_tidy %>% 
  left_join(abstracts[, c("date_published", "title")], by = c("document" = "title"))
```

```{r}
words_by_time <- ac_dfm2_tidy %>%
  mutate(time_floor = floor_date(date_published, unit = "year")) %>%
  filter(time_floor > ymd("2000-01-01")) %>% 
  count(time_floor, term) %>%
  group_by(time_floor) %>%
  mutate(time_total = sum(n)) %>%
  group_by(term) %>%
  mutate(word_total = sum(n)) %>%
  ungroup() %>%
  rename(count = n) %>%
  filter(word_total > 30)
```

```{r}
words_by_time <- words_by_time %>% 
  rename(word = term)
```

```{r}
words_by_time <- words_by_time %>% 
  mutate(prop = count/time_total)
```

First sanity check, just use Julia's logistic regression method before using slingshot's `gam` method.
```{r}
nested_data <- words_by_time %>%
  nest(data = c(time_floor, count, time_total, word_total, prop))
```

```{r}
nested_models <- nested_data %>%
  mutate(models = map(data, ~ glm(cbind(count, time_total) ~ time_floor, ., 
                                  family = "binomial")))
```

```{r}
slopes <- nested_models %>%
  mutate(models = map(models, tidy)) %>%
  unnest(cols = "models") %>%
  filter(term == "time_floor") %>%
  mutate(p_val_adj = p.adjust(p.value, method = "BH"))
```

```{r}
slopes <- slopes %>% 
  select(-data)
```

```{r}
top_slopes <- slopes %>% 
  filter(p_val_adj < 0.001) %>% 
  arrange(p_val_adj)
```

Nothing surprising. 
```{r, fig.width=8, fig.height=8}
words_by_time %>%
  inner_join(top_slopes, by = "word") %>%
  mutate(word = fct_reorder(word, p_val_adj)) %>% 
  ggplot(aes(time_floor, prop)) +
  geom_line() +
  labs(x = "Year", y = "Word frequency") +
  facet_wrap(~ word, ncol = 10)
```

That's a lot. This shows the decline of microarray and the technical stuff and rise of RNA-seq since 2010, which isn't too far from the advent of RNA-seq in 2008. But how about the word "transcriptome"? I think it might be a change in diction, since microarrays do in fact profile the transcriptome. 

Now I try slingshot's GAM method.
```{r}
gam_models <- nested_data %>% 
  mutate(models = map(data, ~ suppressWarnings(gam(prop ~ s(time_floor, 3), data = .))))
```

```{r}
gam_models <- gam_models %>% 
  select(-data) %>% 
  mutate(models = map(models, tidy)) %>%
  unnest(cols = "models") %>%
  filter(term == "s(time_floor, 3)") %>%
  mutate(p_val_adj = p.adjust(p.value, method = "BH"))
```

```{r}
top_gam <- gam_models %>% 
  filter(p_val_adj < 0.001) %>% 
  arrange(p_val_adj)
```

```{r, fig.width=8, fig.height=8}
words_by_time %>%
  inner_join(top_gam, by = "word") %>%
  mutate(word = fct_reorder(word, p_val_adj)) %>% 
  ggplot(aes(time_floor, prop)) +
  geom_line() +
  labs(x = "Year", y = "Word frequency") +
  facet_wrap(~ word, ncol = 6)
```

Yes, the results make sense and is consistent with the logistic regression results. For whatever reason, those are not detected by Julia's logistic regression method. Well, GAM is more flexible. I'll plot a heatmap to see what kind of patterns are there. But meanwhile, since there're too few papers in 2000, I would take some of the results with a grain of salt. Also, what happened to "tumor"? Cancer is still very relevant. I guess it might be due to the rise of other topics like LCM in plants and diseases other than cancer.

```{r}
words_hm <- unique(c(top_gam$word, top_slopes$word))
word_mat_df <- words_by_time %>% 
  filter(word %in% words_hm) %>% 
  mutate(year = year(time_floor)) %>% 
  select(year, word, prop) %>% 
  pivot_wider(id_cols = word, names_from = year, values_from = prop)
```

```{r}
word_mat <- as.matrix(word_mat_df[, -1])
rownames(word_mat) <- word_mat_df$word
```

```{r}
word_mat <- t(scale(t(word_mat)))
```

```{r, fig.width=6, fig.height=6}
heatmap(word_mat, Colv = NA, scale = "none")
```

Which topics are those words associated with?
```{r}
td_beta %>% 
  filter(term == "network") %>% 
  arrange(desc(beta))
```

```{r}
labelTopics(stm_res, 11)
```

```{r}
findThoughts(stm_res, texts = abstracts$abstract, topics = 11, n = 5)
```

