---
title: "Comparisons between sheets"
author: "Lambda Moses"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{comparisons}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Here I compare different sheets and do the analysis for all sheets in the original trilogy together.

```{r}
library(museumst)
library(gganimate)
library(purrr)
library(dplyr)
library(tidytext)
library(tidyr)
theme_set(theme_bw())
```

# Data
How does the trend in number of papers in the original trilogy compare to that in the prequel?
```{r}
nms <- c("Prequel", "Microdissection", "smFISH", "Array", "ISS", "No imaging")
data_sheets <- read_metadata(nms)
```

```{r}
data_sheets <- data_sheets %>% 
  mutate(is_prequel = sheet == "Prequel")
```

See the number of publications; for original trilogy sheets, each dataset has its own row so often one publication has multiple rows.
```{r}
publications <- get_pubs_df(data_sheets, "is_prequel")
```

```{r}
# For maps later in this notebook
c(inst_gc, city_gc) %<-% geocode_inst_city(publications)
pubs_on_map2 <- partial(pubs_on_map, inst_gc = inst_gc, city_gc = city_gc)
```

```{r}
pubs_count <- publications %>% 
  filter(!journal %in% c("bioRxiv", "arXiv")) %>% 
  count(year, is_prequel)
ggplot(pubs_count, aes(year, n, color = is_prequel)) +
  geom_line() +
  scale_y_continuous(limits = c(0, max(pubs_count$n)))
```

Here we see that what's currently known as spatial transcriptomics is growing more than prequel spatial transcriptomics back in the 1990s and 2000s. Or maybe my prequel collection is incomplete, so I'm not so sure how big that peak is. What's more remarkable is the number of publications in 2020. Now it's near the end of May. We're almost half way through the year. Considering that most publications are from Western countries, for which December is the holiday season, I expect this number to double by the end of 2020.

Let me plot these lines not with the absolute year, but how many years since the first publication in the category to see how quickly these things grew. This will definitely change once I make the LCM collection more complete. Since using LCM + transcriptomics started at least as early as 2001 (the oldest paper I can find that did this is from 2001), I would expect the FALSE curve to rise earlier than the TRUE curve.
```{r}
pubs_count <- pubs_count %>% 
  group_by(is_prequel) %>% 
  mutate(years_since_first = year - min(year))
ggplot(pubs_count, aes(years_since_first, n, color = is_prequel)) +
  geom_line() +
  scale_y_continuous(limits = c(0, max(pubs_count$n)))
```

So far they seem pretty parallel (which most likely won't be the case when I make the LCM collection more complete), though I certainly don't think the current era has peaked yet. Based on this, I think the current era will be much greater than the prequel era.

Here I'm making an animation on a map for prequel and original trilogy separately for number of publications per institution. These take a while to run to render all those frames, so not run for CRAN checks.

```{r, eval=FALSE}
world_prequel <- publications %>% 
  filter(is_prequel) %>% 
  pubs_on_map(inst_gc, city_gc, per_year = TRUE)
animate(world_prequel, nframes = 200)
anim_save("output/world_prequel.gif")
```

```{r, eval=FALSE}
europe_prequel <- publications %>% 
  filter(is_prequel) %>% 
  pubs_on_map(inst_gc, city_gc, zoom = "europe", per_year = TRUE)
animate(europe_prequel, nframes = 200)
anim_save("output/europe.gif")
```

```{r, eval=FALSE}
usa_prequel <- publications %>% 
  filter(is_prequel) %>% 
  pubs_on_map(inst_gc, city_gc, zoom = "usa", per_year = TRUE)
animate(usa_prequel, nframes = 200)
anim_save("output/usa.gif")
```

```{r, eval=FALSE}
world_current <- publications %>% 
  filter(!is_prequel) %>% 
  pubs_on_map(inst_gc, city_gc, per_year = TRUE)
animate(world_current, nframes = 150)
anim_save("output/world_current.gif")
```

```{r, eval=FALSE}
europe_current <- publications %>% 
  filter(!is_prequel) %>% 
  pubs_on_map(inst_gc, city_gc, per_year = TRUE, zoom = "europe")
animate(europe_current, nframes = 80)
anim_save("output/europe_current.gif")
```

```{r, eval=FALSE}
usa_current <- publications %>% 
  filter(!is_prequel) %>% 
  pubs_on_map(inst_gc, city_gc, per_year = TRUE, zoom = "usa")
animate(usa_current)
anim_save("output/usa_current.gif")
```

## For all original trilogy data
```{r}
current <- data_sheets %>% 
  filter(!is_prequel) %>% 
  select(date_published:journal, country:year, sheet) %>% 
  distinct()
```

```{r}
current2 <- publications %>% filter(!is_prequel)
```

How many per sheet? Again, I definitely don't think the LCM collection (within Microdissection) is anywhere close to complete, so microdissection is definitely the most popular one so far. Maybe after I make the LCM collection more complete, I should plot the LCM collection separately.
```{r}
pubs_per_cat(current, sheet)
```

Sheet over time
```{r, fig.width=4, fig.height=4}
pubs_per_year(current, "sheet")
```


Species in all the current sheets. For prequels, it has already been plotted in the prequel notebook.
```{r}
species <- data_sheets %>%
  filter(!is_prequel, !is.na(species)) %>% 
  unnest_cat(species)
```

```{r}
pubs_per_cat(species, species)
```

I thought that there would be more mouse papers than human ones, but there're slightly more human ones.

Another fun isotype plot:
```{r}
pubs_per_cat(species, species, n_top = 5, isotype = TRUE, img_unit = 5)
```


```{r}
pubs_per_cat(current2, country)
```

```{r, fig.width=4, fig.height=4}
pubs_per_cat(current2, city)
```

```{r, fig.width=4, fig.height=6}
pubs_per_cat(current2, institution)
```

```{r}
pubs_per_capita(current2)
```

```{r}
pubs_per_capita(current2, plot = "bar")
```

```{r}
pubs_per_capita(current, "europe")
```

```{r}
pubs_per_capita(current, "usa")
```

```{r}
pubs_per_capita(current, "usa", "bar")
```

```{r}
pubs_on_map2(current)
```

Bug: Need to distinguish between Cambridge, UK and Cambridge, MA in bar plots, though this is not a problem on maps.
```{r}
pubs_on_map2(current, zoom = "europe")
```

```{r}
pubs_on_map2(current, zoom = "usa")
```

```{r}
data_sheets %>% 
  filter(!is_prequel) %>% 
  plot_wordcloud()
```

Unlike in Prequel, "database" is not at all prominent here. Which words have the most different frequencies between prequel and current era data papers? Here I use proportion of words to normalize the different sizes of the corpus.
```{r}
word_props <- publications %>% 
  select(title, is_prequel) %>% 
  distinct() %>% 
  unnest_tokens("word", "title") %>% 
  anti_join(stop_words, by = "word") %>% 
  count(is_prequel, word) %>% 
  group_by(is_prequel) %>% 
  mutate(prop = n/sum(n)) %>% 
  pivot_wider(names_from = is_prequel, values_from = prop, values_fill = list(prop = 0)) %>% 
  mutate(dif = abs(`TRUE` - `FALSE`)) %>% 
  top_n(20, dif) %>% 
  pivot_longer(c("TRUE", "FALSE"), names_to = "is_prequel", values_to = "prop") %>% 
  filter(prop > 0) %>% 
  mutate(word = fct_reorder(word, dif))
```

```{r}
word_props %>% 
  ggplot(aes(prop, word, fill = is_prequel)) +
  geom_col(position = position_dodge(0.8)) +
  scale_x_continuous(expand = expansion(c(0, 0.05)))
```

```{r}
data_sheets %>% 
  filter(!is_prequel) %>% 
  plot_wordcloud("tissue")
```

For current era data papers, which programming languages (if any stated in the paper) are the most popular?
```{r}
# prequel sheet doesn't have programming language information anyway though it has that columnn
langs <- unnest_cat(data_sheets, language)
```

```{r}
pubs_per_cat(langs, language, n_top = 5, isotype = TRUE, img_unit = 5)
```

# Analysis
```{r}
nms <- c("Analysis", "Prequel analysis")
analysis_sheets <- read_metadata(nms)
```

Just to see how number of publications changed through time here
```{r}
pubs_per_year(analysis_sheets, "sheet")
```

```{r}
ana_counts <- analysis_sheets %>% 
  filter(!journal %in% c("bioRxiv", "arXiv")) %>% 
  count(sheet, year)
ggplot(ana_counts, aes(year, n, color = sheet)) +
  geom_line() +
  scale_y_continuous(limits = c(0, max(ana_counts$n)))
```

```{r}
ana_counts <- ana_counts %>% 
  group_by(sheet) %>% 
  mutate(years_since_first = year - min(year))
ggplot(ana_counts, aes(years_since_first, n, color = sheet)) +
  geom_line() +
  scale_y_continuous(limits = c(0, max(ana_counts$n)))
```

Again, in the current era, data analysis papers grow more than in the prequel era (if my collection is complete, which I'm not sure). I don't think we have reached the peak yet.

```{r}
sessionInfo()
```
