---
title: "smFISH"
author: "Lambda Moses"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{smFISH}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Here I explore the history and geography of spatial transcriptomics based on smFISH through the metadata I collected.
```{r}
library(museumst)
library(ggplot2)
library(purrr)
library(dplyr)
library(sf)
library(ggrepel)
theme_set(theme_bw())
```

# Import data
By default, the `read_metadata` function reads the appropriate sheet from a cache within this package. This may be outdated, since the Google Sheets are constantly updated. Use the argument `update = TRUE` to download the most up to date version. The default is `FALSE` to avoid API limits during automated CRAN checks.
```{r}
sheet <- read_metadata("smFISH")
```

# Number of publications
How many publications per year?
```{r}
# Remove duplicates as different datasets in the same publication have their own rows
publications <- get_pubs_df(sheet, "repo")
```

How many publications are there in total in this sheet?
```{r}
nrow(publications)
```

```{r}
# For maps later in this notebook
city_gc <- geocode_inst_city(publications)
pubs_on_map2 <- partial(pubs_on_map, city_gc = city_gc)
```

## Overall
Number of publications per year. Preprints are excluded from plots over years as it takes months to publish so the dates of the preprints are incoherent with those for the other papers. 
```{r}
events <- read_major_events()
```

```{r, fig.width=8, fig.height=4}
events %>% 
  filter(category == "smFISH") %>% 
  plot_timeline(c(0.8, -0.5, 0.6, -0.5, 1, 0.6, -0.4, 0.7), 
                expand_x = c(0.1, 0.1), expand_y = c(0.05, 0.05))
```

```{r}
pubs_per_year(publications)
```

## By method
How about when broken down by method (not for data analysis sheets)?

```{r}
methods <- unnest_cat(sheet, method)
```

```{r, fig.width=6, fig.height=12}
pubs_per_year(methods, facet_by = "method")
```

## By species

```{r}
species <- unnest_cat(sheet, species)
```

```{r}
pubs_per_cat(species, species)
```

```{r, fig.width=6, fig.height=3}
pubs_per_cat(species, species, isotype = TRUE, img_unit = 1)
```

## By journal
```{r}
sort(table(publications$journal))
```

## Location
### General
Just some barplots for number of publications per institution, city, and country. 
```{r}
pubs_per_cat(publications, country)
```

How about per capita? Actually this probably does not say much since many researchers in the West are immigrants. Then it raises the burning political question of who counts as the "population".

```{r}
pubs_per_capita(publications, plot = "bar")
```

How about country over time? There might not be enough publications to show a trend.
```{r, fig.height=6, fig.width=6}
pubs_per_year(publications, facet_by = "country")
```

Well, I know, this is because seqFISH comes from Caltech and MERFISH comes from Harvard.

Now look at cities. 

```{r}
pubs_per_cat(publications, city)
```

Cambridge for Harvard, and Pasadena for Caltech.

Institutions
```{r}
pubs_per_cat(publications, institution)
```

So thank you, Long Cai. OK, now here comes the maps!

### Worldwide
```{r}
pubs_on_map2(publications)
```

Let me also plot the per capita thing on a map, as a choropleth
```{r}
pubs_per_capita(publications)
```

Break down by species
```{r}
pubs_on_map2(species, facet_by = "species", ncol = 2)
```

By method
```{r, fig.width=8, fig.height=8}
pubs_on_map2(methods, facet_by = "method")
```

I suppose this merely tells us where those techniques were invented and how they spread. It seems that seqFISH and its variants did not spread beyond Caltech; the dataset labeled Harvard is from a collaboration between Long Cai and someone at Harvard. However, MERFISH did spread beyond Harvard, where it originated, to UCLA.

### Europe
Plot a map just for Europe. A problem here is that if I simply filter the `world` sf object, I'll get some islands away from what we usually think is Europe. Kind of feel bad for Russians since Russia is so large that it makes the map look bad and I don't have a paper from Russia in this spreadsheet. 

```{r}
pubs_on_map2(publications, zoom = "europe")
```

```{r}
pubs_per_capita(publications, "europe")
```

```{r, fig.width=6, fig.height=4}
pubs_on_map2(species, zoom = "europe", facet_by = "species")
```

Maybe I just made this plot for fun. Not sure what to say about it.

```{r, fig.width=6, fig.height=6}
pubs_on_map2(methods, zoom = "europe", facet_by = "method", ncol = 2)
```

Note that HybISS and SCRINSHOT both use RCA, which is used in the original 2013 ISS, which comes from Sweden. Is that why?

### USA
Also a plot just for America. I did not encounter a publication in this spreadsheet that is from Hawaii or Alaska, but I'll include Hawaii and Alaska in the map anyway in case I get one in the future.

```{r}
pubs_on_map2(publications, zoom = "usa")
```

```{r}
pubs_per_capita(publications, "usa")
```

```{r}
pubs_on_map2(species, zoom = "usa", facet_by = "species", ncol = 2)
```

```{r, fig.width=8, fig.height=6}
pubs_on_map2(methods, zoom = "usa", facet_by = "method")
```

# Word cloud
## Titles
```{r, fig.height=6, fig.width=6}
plot_wordcloud(sheet)
```

Yeah, single cell.
```{r, fig.height=6, fig.width=6}
plot_wordcloud(sheet, species_use = "Mus musculus")
```

```{r, fig.height=6, fig.width=6}
plot_wordcloud(sheet, species_use = "Homo sapiens")
```

## Tissues
```{r, fig.height=4, fig.width=4}
plot_wordcloud(sheet, tissue)
```

That 2 comes from U-2 OS, which was used many times to test MERFISH.
```{r, fig.height=6, fig.width=6}
plot_wordcloud(sheet, tissue, species_use = "Mus musculus")
```

```{r, fig.height=4, fig.width=4}
plot_wordcloud(sheet, tissue, species_use = "Homo sapiens")
```

Those are all about cell culture. I look forward to seeing this used on more real tissues.

## Over time

```{r}
range(sheet$date_published)
```

```{r, fig.height=4, fig.width=4}
plot_wordcloud(sheet, year_min = 2012, year_max = 2016)
```

```{r, fig.height=6, fig.width=6}
plot_wordcloud(sheet, year_min = 2016, year_max = 2021)
```

## Department names

```{r, fig.height=6, fig.width=6}
plot_wordcloud(sheet, col_use = "department", other_stop_words = inst_words)
```

That's because Long Cai used to be in CCE, though later he moved to BBE.

## Downstream analyses
```{r, fig.height=5, fig.width=5}
plot_wordcloud(sheet, downstream)
```

# Programming languages
```{r}
langs <- unnest_cat(sheet, language)
```

```{r}
pubs_per_cat(langs, language, isotype = TRUE, img_unit = 2)
```

# Data and code availability

How many publications have provided a code repo?
```{r}
hist_bool(publications, !is.na(repo))
```

Here one paper can have multiple datasets and the availability status of the datasets can be different, so I'm plotting the datasets rather than publications here.

```{r}
hist_bool(sheet, has_matrix)
```

It seems that more recent publications are more likely to provide a repo, but not data. Is this significant? Here I fit a logistic regression model to use date published to predict whether publications have repo or accession and test if the coefficients are 0.
```{r}
test_year_bool(publications, !is.na(repo))
```

Maybe a bit suggestive, but not significant.

# Number of genes and cells

The mean for each study is used since I'm not always sure what counts as a dataset. Sometimes I mean one section, while sometimes I mean multiple sections the authors treated as one dataset. Actually sometimes the authors aren't clear how many sections are there in a "dataset", hence this confusion.
```{r}
mean_genes <- sheet %>% 
  group_by(date_published, title, method) %>% 
  summarize(n_genes = mean(n_genes, na.rm = TRUE))
```

```{r}
ggplot(mean_genes, aes(date_published, n_genes, color = method)) +
  geom_point() +
  geom_text_repel(aes(label = method), segment.alpha = 0.5) +
  labs(x = "Date published", y = "Mean number of genes per study") +
  theme(legend.position = "none")
```

While the max possible number of genes increased, I don't think people necessarily want to go for the max.
```{r}
mean_genes %>% 
  filter(n_genes < 1000) %>% 
  ggplot(aes(date_published, n_genes)) +
  geom_point(aes(color = method)) +
  geom_text_repel(aes(label = method, color = method), segment.alpha = 0.5) +
  geom_smooth(method = "lm") +
  labs(x = "Date published", y = "Mean number of genes per study") +
  theme(legend.position = "none")
```

See, without those 3 outliers, there really does not seem to be an increase (if not a decrease). I wonder why people aren't opting for the larger number of genes possible. Is it because the probes are expensive or it's too challenging to process the sheer number of images, which is exacerbated by poor documentation of much of the existing code written for this task and lack of a unified pipeline? Now I wonder if growth of ST is in part because of STPipeline, which makes data preprocessing easier. Now I also wonder how Cell Ranger and Seurat contributed to spread of scRNA-seq in general.

How about number of cells? Here the total number of cells from each study per method is plotted.
```{r}
sum_cells <- sheet %>% 
  group_by(date_published, title, method) %>% 
  summarize(n_cells = sum(n_cells, na.rm = TRUE)) %>% 
  filter(n_cells > 0)
```

```{r}
ggplot(sum_cells, aes(date_published, n_cells, color = method)) +
  geom_point() +
  geom_text_repel(aes(label = method), segment.alpha = 0.5) +
  labs(x = "Date published", y = "Total number of cells per study") +
  theme(legend.position = "none")
```

That outlier is the crazy hypothalamus MERFISH study.

```{r}
sum_cells2 <- sum_cells %>% 
  filter(n_cells < 1e5)
ggplot(sum_cells2, aes(date_published, n_cells)) +
  geom_point(aes(color = method)) +
  geom_text_repel(aes(label = method, color = method), segment.alpha = 0.5) +
  geom_smooth(method = "lm") +
  labs(x = "Date published", y = "Total number of cells per study") +
  theme(legend.position = "none") +
  coord_cartesian(ylim = c(0, max(sum_cells2$n_cells)))
```

I don't think it's significant. Here I formally test if beta is not 0:
```{r}
summary(lm(n_cells ~ date_published, data = sum_cells2))
```

See, it's not significant. So what does this mean? What should our priorities be? Shall we focus on in depth data analysis techniques without worrying too much about scalability yet since people aren't opting for larger amount of data anyway, or shall we prioritize scalability since lack of scalability of data processing and analysis tools is the reason why people aren't opting for larger amount of data? Or is that because the existing software is not very user friendly, so people consider data analysis a really daunting task? Or is it the expensive reagents and instruments and the amount of time to collect data? Or did people make up their minds and decided that it's not necessary to profile that many cells? I don't know. I think we can try both and see what happens in a few years. Well, given how quickly this field is evolving, I suppose months. 

```{r}
sessionInfo()
```

